@article{Beck2016Visual,
  abstract = {Bibiographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.},
  author = {Beck, Fabian and Koch, Sebastian and Weiskopf, Daniel},
  doi = {10.1109/TVCG.2015.2467757},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  keywords = {visual_analytics, sparklines, information_retrieval, clustering, literature_browser},
  number = {01},
  publisher = {IEEE},
  volume = {22},
  series = {TVCG},
  title = {Visual Analysis and Dissemination of Scientific Literature Collections with {SurVis}},
  url = {http://www.visus.uni-stuttgart.de/uploads/tx_vispublications/vast15-survis.pdf},
  year = {2016}
}
@article {Zhao_2022,
abstract = {Applying advanced technologies such as computer vision is highly desirable in seed testing. Among testing needs, computer vision is a feasible technology for conducting seed and seedling classification used in purity analysis and in germination tests. This review focuses on seed identification
that currently encounters extreme challenges due to a shortage of expertise, time-consuming training and operation, and the need for large numbers of reference specimens. The reviewed computer vision techniques and application strategies also apply to other methods in seed testing. The review
describes the development of machine learning-based computer vision in automating seed identification and their limitations in feature extraction and accuracy. As a subset of machine learning techniques, deep learning has been applied successfully in many agricultural domains, which presents
potential opportunities for its application in seed identification and seed testing. To facilitate application in seed testing, the challenges of deep learning-based computer vision systems are summarised through analysing their application in other agricultural domains. It is recommended
to accelerate the application in seed testing by optimising procedures or approaches in image acquisition technologies, dataset construction and model development. A concept flow chart for using computer vision systems is proposed to advance computer-assisted seed identification.},
author = {Zhao, Liang and Haque, S.M. Rafizul and Wang, Ruojing},
doi = {10.15258/sst.2022.50.1.s.05},
journal = {Seed Science and Technology},
keywords = {SEED TESTING, MACHINE LEARNING, DATASET CONSTRUCTION, IMAGE ANALYSIS, COMPUTER VISION, DEEP LEARNING, SEED IDENTIFICATION, SURVEY, SUMMARISED PAPER},
number = {2},
publisher = {International Seed Testing Association},
volume = {50},
series = {SSAT},
title = {Automated seed identification with computer vision: challenges and opportunities},
url = {https://www.ingentaconnect.com/content/ista/sst/2022/00000050/a00102s1/art00005},
year = {2022},
pages = {75-102},
}

@InProceedings{Labb_e_2020,
author={Labb'e}, Yann
and Carpentier, Justin
and Aubry, Mathieu
and Sivic, Josef",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael},
title={CosyPose: Consistent Multi-view Multi-object 6D Pose Estimation},
booktitle={Computer Vision -- ECCV 2020},
year={2020},
publisher={Springer International Publishing},
address={Cham},
pages={574-591},
abstract={We introduce an approach for recovering the 6D pose of multiple known objects in a scene captured by a set of input images with unknown camera viewpoints. First, we present a single-view single-object 6D pose estimation method, which we use to generate 6D object pose hypotheses. Second, we develop a robust method for matching individual 6D object pose hypotheses across different input images in order to jointly estimate camera viewpoints and 6D poses of all objects in a single consistent scene. Our approach explicitly handles object symmetries, does not require depth measurements, is robust to missing or incorrect object hypotheses, and automatically recovers the number of objects in the scene. Third, we develop a method for global scene refinement given multiple object hypotheses and their correspondences across views. This is achieved by solving an object-level bundle adjustment problem that refines the poses of cameras and objects to minimize the reprojection error in all views. We demonstrate that the proposed method, dubbed CosyPose, outperforms current state-of-the-art results for single-view and multi-view 6D object pose estimation by a large margin on two challenging benchmarks: the YCB-Video and T-LESS datasets. Code and pre-trained models are available on the project webpage. (https://www.di.ens.fr/willow/research/cosypose/.)},
isbn={978-3-030-58520-4},
keywords = {pose esimation, CosyPose, 6D, multi-view, multi-object, summarised paper, computer vision},
}
@article{Liu_2018,
  author={Liu, An-An and Nie, Wei-Zhi and Gao, Yue and Su, Yu-Ting},
  journal={IEEE Transactions on Cybernetics}, 
  title={View-Based 3-D Model Retrieval: A Benchmark}, 
  year={2018},
  volume={48},
  number={3},
  pages={916-928},
  doi={10.1109/TCYB.2017.2664503},
  keywords = {3D object, computer vision, multi-view, multi-object}
  }
@article{Musaev_2019,
doi = {10.1088/1755-1315/395/1/012083},
url = {https://dx.doi.org/10.1088/1755-1315/395/1/012083},
year = {2019},
month = {nov},
publisher = {IOP Publishing},
volume = {395},
number = {1},
pages = {012083},
author = {F Musaev and V Pivovarov and S Beleskyi},
title = {Economic justification for applying instrumental methods of seed quality control},
journal = {IOP Conference Series: Earth and Environmental Science},
abstract = {The basis of food independence of any country is a reliable seed system, and seed control is a concern of the state in many developed countries. The standard methods used for analyzing the quality of seeds do not meet today's requirements for seed production, they are laborious, time-consuming, and also uninformative. The contemporary level of scientific knowledge requires the use of new instrumental methods that are highly informative. We have developed an instrumental X-ray method for assessing the quality of vegetable seeds. It is highly informative, quick and easy to perform, and preserves the material being analyzed. The article compares and economically analyzes the widely used standard morphometric method and the implemented instrumental method of X-ray analysis of seeds. It turned out that the use of instrumental methods for seed quality analysis is economically and energetically justified. The method of X-ray analysis of seeds favorably differs in energy efficiency and the period of performance from the standard method. Introduced instrumental method for visual analysis of radiographs is slightly inferior to the standard in labor costs, but the process is significantly accelerated with automatic analysis. The method is automated, which allows to avoid errors in assessing the quality of seeds associated with the operator's subjectivity. The article calculates the benefit from introducing the X-ray method in the seed control case. It is shown that over a ten-year lifetime of the X-ray diagnostic unit, all the costs of organizing the laboratory pay off even with a relatively small workload and begin to make a profit.},
keywords = {economic, seed identification, finance, world state, seed production, challenges}
}
@article{Dell_Aquila_2007,
author={Dell'Aquila, A.},
title={Pepper seed germination assessed by combined X-radiography and computer-aided imaging analysis},
journal={Biologia Plantarum},
year={2007},
month={Dec},
day={01},
volume={51},
number={4},
pages={777-781},
abstract={A lot of pepper seeds having 87 {\%} germination were subjected to X-ray inspection using a non lethal dose of radiation. Seeds with less than 2.7 {\%} (on the basis of total seed area) of free space area, i.e. the spaces between embryo and endosperm, were classified as highly viable seeds (97--100 {\%} germination) with the lowest level of abnormal seedlings. Seeds X-ray classified as good were subjected to a computerised image analysis to study seed imbibition and radicle elongation. The patterns of seed area increase, chosen as the most accurate indicator of seed swelling, resembled the triphasic curve of water uptake. The first phase was completed at 9 h followed by a second phase that varied widely in time until completion of germination between 52 and 96 h. The proportion of seeds with radicle protrusion between 52--56 h and 64--72 h assessed with the image analysis was significantly higher than that recorded using a conventional germination test. In addition, the rate of increase of seed area during the third phase of imbibition, mostly due to protrusion of the radicle tip and its growth, was highly correlated with the corresponding radicle elongation rate.},
issn={1573-8264},
doi={10.1007/s10535-007-0159-9},
url={https://doi.org/10.1007/s10535-007-0159-9},
keywords = {computer vision, X-ray, seed identification, seed germination}
}

@article{Lohumi_2013,
  author    = {LohumiSantosh and MoChangyeun and KangJum-Soon and HongSoon-Jung and ChoByoung-Kwan},
  title     = {Nondestructive Evaluation for the Viability of Watermelon (Citrullus lanatus) Seeds Using Fourier Transform Near Infrared Spectroscopy},
  journal   = {Journal of Biosystems Engineering},
  volume    = {38},
  number    = {4},
  pages     = {312-317},
  year      = {2013},
  month     = {12},
  abstract = {Purpose: Conventional methods used to evaluate seeds viability are destructive, time consuming, and require the use of
chemicals, which are not feasible to implement to process plant in seed industry. In this study, the effectiveness of Fourier
transform near infrared (FT-NIR) spectroscopy to differentiate between viable and nonviable watermelon seeds was
investigated. Methods: FT-NIR reflectance spectra of both viable and non-viable (aging) seeds were collected in the range of
4,000 - 10,000 cm-1 (1,000 - 2,500 nm). To differentiate between viable and non-viable seeds, a multivariate classification
model was developed with partial least square discrimination analysis (PLS-DA). Results: The calibration and validation set
derived from the PLS-DA model classified viable and non-viable seeds with 100% accuracy. The beta coefficient of PLS-DA,
which represented spectral difference between viable and non-viable seeds, showed that change in the chemical component
of the seed membrane (such as lipids and proteins) might be responsible for the germination ability of the seeds.
Conclusions: The results demonstrate the possibility of using FT-NIR spectroscopy to separate seeds based on viability,
which could be used in the development of an online sorting technique},
doi = {https://doi.org/10.5307/JBE.2013.38.4.312},
url = {https://koreascience.kr/article/JAKO201336447764748.page},
keywords = {seed identification, Infrared, Fourier}
}
@article{Sabanci_2017,
author = {Sabanci, Kadir and Kayabasi, Ahmet and Toktas, Abdurrahim},
title = {Computer vision-based method for classification of wheat grains using artificial neural network},
journal = {Journal of the Science of Food and Agriculture},
volume = {97},
number = {8},
pages = {2588-2593},
keywords = {seed classification, wheat grains, image processing, artificial neural network, ANN, NN, multilayer perceptron, computer vision},
doi = {https://doi.org/10.1002/jsfa.8080},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jsfa.8080},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jsfa.8080},
abstract = {Abstract BACKGROUND A simplified computer vision-based application using artificial neural network (ANN) depending on multilayer perceptron (MLP) for accurately classifying wheat grains into bread or durum is presented. The images of 100 bread and 100 durum wheat grains are taken via a high-resolution camera and subjected to pre-processing. The main visual features of four dimensions, three colors and five textures are acquired using image-processing techniques (IPTs). A total of 21 visual features are reproduced from the 12 main features to diversify the input population for training and testing the ANN model. The data sets of visual features are considered as input parameters of the ANN model. The ANN with four different input data subsets is modelled to classify the wheat grains into bread or durum. The ANN model is trained with 180 grains and its accuracy tested with 20 grains from a total of 200 wheat grains. RESULTS Seven input parameters that are most effective on the classifying results are determined using the correlation-based CfsSubsetEval algorithm to simplify the ANN model. The results of the ANN model are compared in terms of accuracy rate. The best result is achieved with a mean absolute error (MAE) of 9.8 × 10−6 by the simplified ANN model. CONCLUSION This shows that the proposed classifier based on computer vision can be successfully exploited to automatically classify a variety of grains. © 2016 Society of Chemical Industry},
year = {2017}
}
@article{Zhang_2020, 
title={Applications of Deep Learning for Dense Scenes Analysis in Agriculture: A Review}, 
volume={20}, 
ISSN={1424-8220}, 
url={http://dx.doi.org/10.3390/s20051520}, 
doi={https://doi.org/10.3390/s20051520}, 
number={5}, 
journal={Sensors}, 
publisher={MDPI AG}, 
author={Zhang, Qian and Liu, Yeqi and Gong, Chuanyang and Chen, Yingyi and Yu, Huihui}, 
year={2020}, 
month={Mar}, 
pages={1520}, 
keywords = {deep learning, computer vision, review, agriculture}
}
@article{Eryigit_2021,
title={Performance of Various Deep-Learning Networks in the Seed Classification Problem}, 
volume={13}, 
ISSN={2073-8994}, 
url={http://dx.doi.org/10.3390/sym13101892}, 
DOI={10.3390/sym13101892}, 
number={10}, 
journal={Symmetry}, 
publisher={MDPI AG}, 
author={Eryigit, Recep and Tugrul, Bulent}, 
year={2021}, 
month={Oct}, 
pages={1892},
keywords = {deep learning, computer vision, seed classification, summarised paper, convolutional neural network, CNN, best methods}
}
@article{Krizhevsky_2017,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
journal = {Commun. ACM},
month = {may},
pages = {84-90},
numpages = {7},
keywords = {computer vision, deep learning,convolutional neural network, CNN, ImageNet}
}
@article{Tajbakhsh_2016,
  author={Tajbakhsh, Nima and Shin, Jae Y. and Gurudu, Suryakanth R. and Hurst, R. Todd and Kendall, Christopher B. and Gotway, Michael B. and Liang, Jianming},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?}, 
  year={2016},
  volume={35},
  number={5},
  pages={1299-1312},
  doi={https://doi.org/10.1109/TMI.2016.2535302},
  keywords = {computer vision, medical, convolutional neural network, CNN, fine tuning}
  }
@article{Too_2019,
title = {A comparative study of fine-tuning deep learning models for plant disease identification},
journal = {Computers and Electronics in Agriculture},
volume = {161},
pages = {272-279},
year = {2019},
note = {BigData and DSS in Agriculture},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.03.032},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917313303},
author = {Edna Chebet Too and Li Yujian and Sam Njuki and Liu Yingchun},
keywords = {Plant disease identification, deep learning, convolutional neural networks, CNN, Image recognition, fine tuning, computer vision},
abstract = {Deep learning has recently attracted a lot of attention with the aim to develop a quick, automatic and accurate system for image identification and classification. In this work, the focus was on fine-tuning and evaluation of state-of-the-art deep convolutional neural network for image-based plant disease classification. An empirical comparison of the deep learning architecture is done. The architectures evaluated include VGG 16, Inception V4, ResNet with 50, 101 and 152 layers and DenseNets with 121 layers. The data used for the experiment is 38 different classes including diseased and healthy images of leafs of 14 plants from plantVillage. Fast and accurate models for plant disease identification are desired so that accurate measures can be applied early. Thus, alleviating the problem of food security. In our experiment, DenseNets has tendency’s to consistently improve in accuracy with growing number of epochs, with no signs of overfitting and performance deterioration. Moreover, DenseNets requires a considerably less number of parameters and reasonable computing time to achieve state-of-the-art performances. It achieves a testing accuracy score of 99.75% to beat the rest of the architectures. Keras with Theano backend was used to perform the training of the architectures.}
}
@article{Loddo_2021,
title = {A novel deep learning based approach for seed image classification and retrieval},
journal = {Computers and Electronics in Agriculture},
volume = {187},
pages = {106269},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106269},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921002866},
author = {Andrea Loddo and Mauro Loddo and Cecilia {Di Ruberto}},
keywords = {Agriculture, deep learning, Machine learning, Image analysis, Seed classification, Retrieval, computer vision},
abstract = {Seeds image analysis has become essential to preserve biodiversity. This is why recognition and classification of plant species on the earth's planet is nowadays a great challenge. The paper focuses on this purpose by studying two plant seeds datasets to classify their families or species through deep learning techniques. SeedNet, a novel CNN has been proposed to face the depicted issue, and several state-of-the-art convolutional neural networks have been exploited for an exhaustive comparison of most adequate for the considered scenario. In detail, promising results in seed classification for both analysed datasets, reaching accuracy values of 95.65% for the first one and 97.47% for the second one, have been obtained. The retrieval problem with the deep learning approach was also addressed, achieving satisfying performances. We consider the obtained results for both the tasks as an excellent starting point to develop a complete seeds recognition, classification and retrieval system to offer impressive support in agriculture and botany fields.}
}
@article{Sabanci_2022,
author = {Sabanci, Kadir and Aslan, Muhammet Fatih and Ropelewska, Ewa and Unlersen, Muhammed Fahri},
title = {A convolutional neural network-based comparative study for pepper seed classification: Analysis of selected deep features with support vector machine},
journal = {Journal of Food Process Engineering},
volume = {45},
number = {6},
pages = {e13955},
doi = {https://doi.org/10.1111/jfpe.13955},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jfpe.13955},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/jfpe.13955},
abstract = {Abstract The seeds of high quality are very important for the cultivation of the pepper. The required cultivation practices and growing conditions may be affected by the cultivar. Also, the productivity and properties of pepper depend on the cultivar. The selection of appropriate seed cultivars may be necessary for the breeding programs. The cultivar differentiation of pepper seeds may be tested by the human eye. However, small sizes and visual similarities make it difficult to distinguish between seed cultivars. Computer vision and artificial intelligence can provide high cultivar discrimination accuracy and the procedures are objective and fast. This study aimed to classify pepper seeds belonging to different cultivars with convolutional neural network (CNN) models. The seeds were obtained from green, orange, red, and yellow pepper cultivars. A flatbed scanner was used to acquire the pepper seed images. After the image acquisition, the procedure applied was preprocessing of the images, data augmentation using different techniques and then deep learning-based classification. Two approaches have been proposed for classification. In the first approach, CNN models (ResNet18 and ResNet50) were trained for pepper seeds. In the second approach, different from the first, the features of pretrained CNN models were fused, and feature selection was applied to the fused features. Classification using all features and selected features was performed with the support vector machine (SVM) with different kernel functions (Linear, Quadratic, Cubic, Gaussian). The accuracies in the first approximation were 98.05\% and 97.07\% for ResNet50 and ResNet18, respectively. In the second approach, CNN-SVM-Cubic achieved up to 99.02\% accuracy with the selected features. Practical applications In precision agriculture, it is very important that the seeds be of the same type for the purification and standardization of the crop culture. Performing this classification manually with human assistance will result in subjective, slow, and low standard outcomes. To overcome such problems, classification supported by artificial intelligence and machine vision systems emerges as an important tool. In this study, a highly successful classification system is presented according to the visual characteristics of pepper seeds. The proposed models can be preferred in practice for identifying pepper seeds and detecting falsification or ensuring their reliability. It will prevent mixing of different pepper seeds with different attributes for processing.},
year = {2022},
keywords = {summarised paper, computer vision, best methods, convolutional neural network, CNN, machine learning, seed classification}
}

@article{Pornpanomchai__2020, 
title={Image analysis on color and texture for chili (Capsicum frutescence) seed germination}, 
volume={14}, 
url={https://li01.tci-thaijo.org/index.php/sehs/article/view/238650}, 
doi={10.14456/sehs.2020.16}, 
abstract={The objective of this research was to develop a computer system for evaluating the germination of a chili seed. The system called "chili seed germination analysis or (CSGA)", evaluated the bird's eye chili (Capsicum frutescence) seed and long fed pepper (Capsicum annuum acuminatum Fingerh.) seed germination by using an image processing technique. The chili seed images were taken by a mobile phone camera with 60 microscope. The CSGA consisted of six main modules, namely 1) image acquisition, 2) seed image segmentation, 3) feature extraction, 4) germination evaluation, 5) result presentation and 6) germination verification. The CSGA employed color and texture features of chili seed images to evaluate the germination. The system applied the Euclidean distance and a neural network technique to perform the system evaluation. The system precision rates were 59.71% and 71.71% for Euclidean distance and a neural network technique, respectively. The average access time was 0.74 seconds/image for the Euclidean distance and 3.54 seconds/image for the neural network technique.}, 
number={3}, 
journal={Science, Engineering and Health Studies}, 
author={Pornpanomchai, Chomtip and Jongsriwattanaporn, Sirapat and Pattanakul, Taviporn and Suriyun, Witchayaporn}, 
year={2020}, 
month={Sep.},
pages={169-183},
keywords={summarised paper, computer vision, neural network, NN, Euclidean Distance, full system}
}
@article{Li_2020,
author={Li, Xingwang
and Fan, Xiaofei
and Zhao, Lili
and Huang, Sheng
and He, Yi
and Suo, Xuesong},
title={Discrimination of Pepper Seed Varieties by Multispectral Imaging Combined with Machine Learning},
journal={Applied Engineering in Agriculture},
year={2020},
publisher={ASABE},
address={St. Joseph, MI},
volume={36},
number={5},
pages={743-749},
keywords={Multispectral imaging, cnn, computer vision, seed classification, One-dimensional convolutional neural network, Pepper seed},
abstract={HighlightsThis study revealed the feasibility of to classify pepper seed varieties using multispectral imaging combined with one-dimensional convolutional neural network (1D-CNN).Convolutional neural networks were adopted to develop models for prediction of seed varieties, and the performance was compared with KNN and SVM.In this experiment, the classification effect of the SVM classification model is the best, but the 1D-CNN classification model is relatively easy to implement. When non-seed materials are mixed in seeds or seed varieties of low value are mixed in high value varieties, it will cause losses to growers or businesses. Thus, the successful discrimination of seed varieties is critical for improvement of seed ralue. In recent years, convolutional neural networks (CNNs) have been used in classification of seed varieties. The feasibility of using multispectral imaging combined with one-dimensional convolutional neural network (1D-CNN) to classify pepper seed varieties was studied. The total number of three varieties of samples was 1472, and the average spectral curve between 365nm and 970nm of the three varieties was studied. The data were analyzed using full bands of the spectrum or the feature bands selected by successive projection algorithm (SPA). SPA extracted 9 feature bands from 19 bands (430, 450, 470, 490, 515, 570, 660, 780, and 880 nm). The classification accuracy of the three classification models developed with full band using K nearest neighbors (KNN), support vector machine (SVM), and 1D-CNN were 85.81{\%}, 97.70{\%}, and 90.50{\%}, respectively. With full bands, SVM and 1D-CNN performed significantly better than KNN, and SVM performed slightly better than 1D-CNN. With feature bands, the testing accuracies of SVM and 1D-CNN were 97.30{\%} and 92.6{\%}, respectively. Although the classification accuracy of 1D-CNN was not the highest, the ease of operation made it the most feasible method for pepper seed variety prediction.},
issn={0883-8542},
url={https://elibrary.asabe.org/abstract.asp?aid=51828&t=3}
}

@article{Bayram_2023,
	journal = {Journal of Agricultural Sciences},
  title = {Classification of Some Barley Cultivars with Deep Convolutional Neural Networks},
	author = {Fatih Bayram and Mustafa Yildiz},
  abstract = {The homogeneity of the seeds is an important factor in terms of processing, transportation, storage, and product quality of agricultural products. It is possible to classify the grain polymorphism of barley cultivars, which are economically important among cereal crops, in a short time with computer vision methods with high accuracy rate and almost zero cost. In this research, a novel image database consisting of 2800 images were created to classify 14 barley cultivars. Six different deep convolutional neural network models were designed based on a transfer learning method with pretrained DenseNet-121, DenseNet-169, DenseNet-201, InceptionResNetV2, MobileNetV2 and Xception networks. The models were trained and evaluated with test-time augmentation method, the best performance was obtained from DenseNet-169 model with average 96.07% recall, 96.29% precision, 96.07% F1-score, and 96.07% accuracy on a test set independent of the training set. The results showed that the transfer learning method performed using additional layers such as dropout and data augmentation with sufficient data samples in these images with high similarities prevented overfitting by increasing the model performance. As a result, it can be suggested that the provided web tool based on the transfer model has an encouraging performance in identifying seeds with a high number of cultivars such as barley.},
	doi = {10.15832/ankutbd.815230},
  publisher = {Ankara University},
	year = {2023},
	volume = {29},
	number = {1},
	pages = {262-271},
  keywords = {summarised paper, best methods, SEED TESTING, MACHINE LEARNING, Transfer learning, IMAGE ANALYSIS, COMPUTER VISION, DEEP LEARNING, SEED IDENTIFICATION, CNN, computer vision},
  url = {https://dergipark.org.tr/en/pub/ankutbd/issue/75312/815230},
}

@InProceedings{Huang_2017,
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Densely Connected Convolutional Networks}, 
  year={2017},
  pages={2261-2269},
  doi={https://doi.org/10.1109/CVPR.2017.243},
  keywords = {convolutional neural network, cnn, deep learning, dense CNN, computer vision, pattern recognition}
  }

@article{Colmer_2020,
author = {Colmer, Joshua and O'Neill, Carmel M. and Wells, Rachel and Bostrom, Aaron and Reynolds, Daniel and Websdale, Danny and Shiralagi, Gagan and Lu, Wei and Lou, Qiaojun and Le Cornu, Thomas and Ball, Joshua and Renema, Jim and Flores Andaluz, Gema and Benjamins, Rene and Penfield, Steven and Zhou, Ji},
title = {SeedGerm: a cost-effective phenotyping platform for automated seed imaging and machine-learning based phenotypic analysis of crop seed germination},
journal = {New Phytologist},
volume = {228},
number = {2},
pages = {778-793},
keywords = {summarised paper, full system, big data biology, crop seeds, germination scoring, machine learning, phenotypic analysis, seed germination, seed imaging, seed classification, computer vision},
doi = {https://doi.org/10.1111/nph.16736},
url = {https://nph.onlinelibrary.wiley.com/doi/abs/10.1111/nph.16736},
eprint = {https://nph.onlinelibrary.wiley.com/doi/pdf/10.1111/nph.16736},
abstract = {Summary Efficient seed germination and establishment are important traits for field and glasshouse crops. Large-scale germination experiments are laborious and prone to observer errors, leading to the necessity for automated methods. We experimented with five crop species, including tomato, pepper, Brassica, barley, and maize, and concluded an approach for large-scale germination scoring. Here, we present the SeedGerm system, which combines cost-effective hardware and open-source software for seed germination experiments, automated seed imaging, and machine-learning based phenotypic analysis. The software can process multiple image series simultaneously and produce reliable analysis of germination- and establishment-related traits, in both comma-separated values (CSV) and processed images (PNG) formats. In this article, we describe the hardware and software design in detail. We also demonstrate that SeedGerm could match specialists' scoring of radicle emergence. Germination curves were produced based on seed-level germination timing and rates rather than a fitted curve. In particular, by scoring germination across a diverse panel of Brassica napus varieties, SeedGerm implicates a gene important in abscisic acid (ABA) signalling in seeds. We compared SeedGerm with existing methods and concluded that it could have wide utilities in large-scale seed phenotyping and testing, for both research and routine seed technology applications.},
year = {2020}
}
@article{Lurstwut_2017,
title = {Image analysis based on color, shape and texture for rice seed (Oryza sativa L.) germination evaluation},
journal = {Agriculture and Natural Resources},
volume = {51},
number = {5},
pages = {383-389},
year = {2017},
issn = {2452-316X},
doi = {https://doi.org/10.1016/j.anres.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2452316X17306361},
author = {Benjamaporn Lurstwut and Chomtip Pornpanomchai},
keywords = {Artificial neural network, ANN, Digital image processing, Germination evaluation, Germination verification, Rice seeds, seed classification, computer vision},
abstract = {Computer software-the Rice Seed Germination Evaluation System (RSGES)-was developed which can evaluate a rice seed image for germination prediction by using digital image processing and an artificial neural networks technique. The digital images are taken with a normal digital camera or mobile phone camera, which is very easy for farmers to process. RSGES consists of six main processing modules: 1) image acquisition, 2) image preprocessing, 3) feature extraction, 4) germination evaluation, 5) results presentation and 6) germination verification. The experiment was conducted on seed of the Thai rice species CP-111 in Bangkok and Chiang Mai, Thailand. RSGES extracted 18 features: 3 color features, 7 morphological features and 8 textural features. The system applied artificial neural network techniques to perform germination prediction. The system precision rate was 7.66% false accepted and 5.42% false rejected, with a processing speed of 8.31s per image.}
}
@article{Skrubej_2015,
  title={Assessment of germination rate of the tomato seeds using image processing and machine learning},
  author={Skrubej, U and Rozman, C and Stajnko, D and others},
  journal={European Journal of Horticultural Science},
  volume={80},
  number={2},
  pages={68-75},
  year={2015},
  publisher={International Society for Horticultural Science (ISHS)},
  doi = {http://dx.doi.org/10.17660/eJHS.2015/80.2.4},
  url = {https://www.pubhort.org/ejhs/80/2/4/},
  keywords = {seed germination, computer vision, machine learning, seed classification}
}
@article{Dell_Aquila_2009,
  title={New perspectives for seed germination testing through digital imaging technology},
  author={Dell'Aquila, Antonio},
  journal={The open agriculture journal},
  volume={3},
  number={1},
  year={2009},
  url = {https://openagriculturejournal.com/VOLUME/3/PAGE/37/ABSTRACT/},
  doi = {http://dx.doi.org/10.2174/1874331500903010037},
  keywords = {seed germination, computer vision, seed classification}
}
@article{Watson_2018,
author={Watson, Amy
and Ghosh, Sreya
and Williams, Matthew J.
and Cuddy, William S.
and Simmonds, James
and Rey, Mar{\'i}a-Dolores
and Asyraf Md Hatta, M.
and Hinchliffe, Alison
and Steed, Andrew
and Reynolds, Daniel
and Adamski, Nikolai M.
and Breakspear, Andy
and Korolev, Andrey
and Rayner, Tracey
and Dixon, Laura E.
and Riaz, Adnan
and Martin, William
and Ryan, Merrill
and Edwards, David
and Batley, Jacqueline
and Raman, Harsh
and Carter, Jeremy
and Rogers, Christian
and Domoney, Claire
and Moore, Graham
and Harwood, Wendy
and Nicholson, Paul
and Dieters, Mark J.
and DeLacy, Ian H.
and Zhou, Ji
and Uauy, Cristobal
and Boden, Scott A.
and Park, Robert F.
and Wulff, Brande B. H.
and Hickey, Lee T.},
title={Speed breeding is a powerful tool to accelerate crop research and breeding},
journal={Nature Plants},
year={2018},
month={Jan},
day={01},
volume={4},
number={1},
pages={23-29},
abstract={The growing human population and a changing environment have raised significant concern for global food security, with the current improvement rate of several important crops inadequate to meet future demand. This slow improvement rate is attributed partly to the long generation times of crop plants. Here, we present a method called `speed breeding', which greatly shortens generation time and accelerates breeding and research programmes. Speed breeding can be used to achieve up to 6 generations per year for spring wheat (Triticum aestivum), durum wheat (T. durum), barley (Hordeum vulgare), chickpea (Cicer arietinum) and pea (Pisum sativum), and 4 generations for canola (Brassica napus), instead of 2--3 under normal glasshouse conditions. We demonstrate that speed breeding in fully enclosed, controlled-environment growth chambers can accelerate plant development for research purposes, including phenotyping of adult plant traits, mutant studies and transformation. The use of supplemental lighting in a glasshouse environment allows rapid generation cycling through single seed descent (SSD) and potential for adaptation to larger-scale crop improvement programs. Cost saving through light-emitting diode (LED) supplemental lighting is also outlined. We envisage great potential for integrating speed breeding with other modern crop breeding technologies, including high-throughput genotyping, genome editing and genomic selection, accelerating the rate of crop improvement.},
issn={2055-0278},
doi={10.1038/s41477-017-0083-8},
url={https://doi.org/10.1038/s41477-017-0083-8},
keywords = {seed germination, food security, seed industry challenges}
}
@article{Zhou_2017a,
author={Zhou, Ji
and Applegate, Christopher
and Alonso, Albor Dobon
and Reynolds, Daniel
and Orford, Simon
and Mackiewicz, Michal
and Griffiths, Simon
and Penfield, Steven
and Pullen, Nick},
title={Leaf-GP: an open and automated software application for measuring growth phenotypes for arabidopsis and wheat},
journal={Plant Methods},
year={2017},
month={Dec},
day={22},
volume={13},
number={1},
pages={117},
abstract={Plants demonstrate dynamic growth phenotypes that are determined by genetic and environmental factors. Phenotypic analysis of growth features over time is a key approach to understand how plants interact with environmental change as well as respond to different treatments. Although the importance of measuring dynamic growth traits is widely recognised, available open software tools are limited in terms of batch image processing, multiple traits analyses, software usability and cross-referencing results between experiments, making automated phenotypic analysis problematic.},
issn={1746-4811},
doi={10.1186/s13007-017-0266-3},
url={https://doi.org/10.1186/s13007-017-0266-3},
keywords = {plant phenotyping, computer vision, machine learning, full systems}
}

@article {Zhou_2017b,
	author = {Ji Zhou and Daniel Reynolds and Thomas Le Cornu and Danny Websdale and Simon Orford and Clare Lister and Oscar Gonzalez-Navarro and Stephen Laycock and Graham Finlayson and Tim Stitt and Matthew D. Clark and Michael W. Bevan and Simon Griffiths},
	title = {CropQuant: An automated and scalable field phenotyping platform for crop monitoring and trait measurements to facilitate breeding and digital agriculture},
	elocation-id = {161547},
	year = {2017},
	doi = {10.1101/161547},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Automated phenotyping technologies are capable of providing continuous and precise measurements of traits that are key to todays crop research, breeding and agronomic practices. In additional to monitoring developmental changes, high-frequency and high-precision phenotypic analysis can enable both accurate delineation of the genotype-to-phenotype pathway and the identification of genetic variation influencing environmental adaptation and yield potential. Here, we present an automated and scalable field phenotyping platform called CropQuant, designed for easy and cost-effective deployment in different environments. To manage infield experiments and crop-climate data collection, we have also developed a web-based control system called CropMonitor to provide a unified graphical user interface (GUI) to enable realtime interactions between users and their experiments. Furthermore, we established a high-throughput trait analysis pipeline for phenotypic analyses so that lightweight machine-learning modelling can be executed on CropQuant workstations to study the dynamic interactions between genotypes (G), phenotypes (P), and environmental factors (E). We have used these technologies since 2015 and reported results generated in 2015 and 2016 field experiments, including developmental profiles of five wheat genotypes, performance-related traits analyses, and new biological insights emerged from the application of the CropQuant platform.},
	URL = {https://www.biorxiv.org/content/early/2017/09/01/161547},
	eprint = {https://www.biorxiv.org/content/early/2017/09/01/161547.full.pdf},
	journal = {bioRxiv},
  keywords = {plant phenotyping, CropQuant, full system, computer vision}
}
@Article{Ma_2021,
author={Ma, Jiayi
and Jiang, Xingyu
and Fan, Aoxiang
and Jiang, Junjun
and Yan, Junchi},
title={Image Matching from Handcrafted to Deep Features: A Survey},
journal={International Journal of Computer Vision},
year={2021},
month={Jan},
day={01},
volume={129},
number={1},
pages={23-79},
abstract={As a fundamental and critical task in various visual applications, image matching can identify then correspond the same or similar structure/content from two or more images. Over the past decades, growing amount and diversity of methods have been proposed for image matching, particularly with the development of deep learning techniques over the recent years. However, it may leave several open questions about which method would be a suitable choice for specific applications with respect to different scenarios and task requirements and how to design better image matching methods with superior performance in accuracy, robustness and efficiency. This encourages us to conduct a comprehensive and systematic review and analysis for those classical and latest techniques. Following the feature-based image matching pipeline, we first introduce feature detection, description, and matching techniques from handcrafted methods to trainable ones and provide an analysis of the development of these methods in theory and practice. Secondly, we briefly introduce several typical image matching-based applications for a comprehensive understanding of the significance of image matching. In addition, we also provide a comprehensive and objective comparison of these classical and latest techniques through extensive experiments on representative datasets. Finally, we conclude with the current status of image matching technologies and deliver insightful discussions and prospects for future works. This survey can serve as a reference for (but not limited to) researchers and engineers in image matching and related fields.},
issn={1573-1405},
doi={10.1007/s11263-020-01359-2},
url={https://doi.org/10.1007/s11263-020-01359-2},
keywords = {summarised paper, computer vision, machine learning, image matching, multi-view, multi-object, survey, review, challenges, opportunities}
}

@INPROCEEDINGS{Xie_2021,
  author={Xie, Youye and Tang, Yingheng and Tang, Gongguo and Hoff, William},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Learning To Find Good Correspondences Of Multiple Objects}, 
  year={2021},
  pages={2779-2786},
  doi={10.1109/ICPR48806.2021.9413319},
  keywords = {summarised paper, computer vision, pose estimation, correspondence, image matching, 3D view, multi-view}
  }

@article{Liu_2021,
title = {Hierarchical multi-view context modelling for 3D object classification and retrieval},
journal = {Information Sciences},
volume = {547},
pages = {984-995},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.09.057},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520309671},
author = {An-An Liu and Heyu Zhou and Weizhi Nie and Zhenguang Liu and Wu Liu and Hongtao Xie and Zhendong Mao and Xuanya Li and Dan Song},
keywords = {summarised paper, multi-view, 3D object retrieval, 3D object classification, computer vision, convolutional neural network},
abstract = {Recent advances in 3D sensors and 3D modelling software have led to big 3D data. 3D object classification and retrieval are becoming important but challenging tasks. One critical problem for them is how to learn the discriminative multi-view visual characteristics. To address it, we proposes a hierarchical multi-view context modelling method (HMVCM). It consists of four key modules. First, the module of view-level context learning is designed to learn visual context features with respect to individual views and their neighbours. This module can imitate the human need to look back and forth to identify and compare the discriminative parts of individual 3D objects based on a joint convolutional neural network (CNN) and bidirectional long short-term memory (Bi-LSTM) network. Then, a multi-view grouping module is introduced to split views into several groups based on their visual appearance. A raw group-level representation can be obtained by the weighted sum of the view-level descriptors. Furthermore, we employ the Bi-LSTM to exploit the context among adjacent groups to generate group-wise context features. Finally, all group-wise context features are fused into a compact 3D object descriptor according to their significance. Extensive experiments on ModelNet10, ModelNet40 and ShapeNetCore55 demonstrate the superiority of the proposed method.}
}

@INPROCEEDINGS{Bachmann_2019,
  author={Bachmann, Roman and Sporri, Jorg and Fua, Pascal and Rhodin, Helge},
  booktitle={2019 International Conference on 3D Vision (3DV)}, 
  title={Motion Capture from Pan-Tilt Cameras with Unknown Orientation}, 
  year={2019},
  pages={308-317},
  doi={10.1109/3DV.2019.00042},
  keywords = {3D object, 3D vision, computer vision, multi-view, unkown camera orientation}
  }
 @article{Fischler_1981,
author = {Fischler, Martin A. and Bolles, Robert C.},
title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
year = {1981},
issue_date = {June 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/358669.358692},
doi = {10.1145/358669.358692},
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
journal = {Commun. ACM},
month = {jun},
pages = {381-395},
numpages = {15},
keywords = {model fitting, location determination, scene analysis, image matching, camera calibration, automated cartography, RANSAC}
}
@InProceedings{Moo_2018,
author = {Yi, Kwang Moo and Trulls, Eduard and Ono, Yuki and Lepetit, Vincent and Salzmann, Mathieu and Fua, Pascal},
title = {Learning to Find Good Correspondences},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018},
keywords = {correspondence, image matching, computer vision}
}
@InProceedings{Dang_2018,
author = {Dang, Zheng and Yi, Kwang Moo and Hu, Yinlin and Wang, Fei and Fua, Pascal and Salzmann, Mathieu},
title = {Eigendecomposition-free Training of Deep Networks with Zero Eigenvalue-based Losses},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018},
keywords = {Eigendecomposition, deep learning, computer vision, convolutional neural networks, cnn}
}
@article{Muwei_2019,
title = {Multi-view face hallucination using SVD and a mapping model},
journal = {Information Sciences},
volume = {488},
pages = {181-189},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519302245},
author = {Muwei Jian and Chaoran Cui and Xiushan Nie and Huaxiang Zhang and Liqiang Nie and Yilong Yin},
keywords = {Face hallucination, Singular value decomposition, multi-view, Mapping model},
abstract = {Multi-view face hallucination (MFH) presents a challenge issue in face recognition domain. In this paper, an efficient method based on singular value decomposition (SVD) and a mapping model is proposed for multi-view face hallucination. Based on an approximately same linear mapping relationship across different views, two corresponding matrices obtained from the SVD of the low resolution (LR) image for the high-resolution (HR) multi-view face images can be constructed via the mapping model using global reconstruction. Experiments show that our proposed multi-view face-hallucination scheme is effective and produces promising super-resolved results.}
}









